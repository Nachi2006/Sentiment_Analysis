{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8ynnBZ21ZXdXrKQBYncos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nachi2006/Sentiment_Analysis/blob/main/Sentiment_analyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOere45Yn9Ls",
        "outputId": "66dad59a-fc2d-42f3-8a75-7b44d2826145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5842 entries, 0 to 5841\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Sentence   5842 non-null   object\n",
            " 1   Sentiment  5842 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 91.4+ KB\n",
            "None\n",
            "\n",
            "First few rows of the dataset:\n",
            "                                            Sentence Sentiment\n",
            "0  The GeoSolutions technology will leverage Bene...  positive\n",
            "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
            "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
            "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
            "4  The Swedish buyout firm has sold its remaining...   neutral\n",
            "\n",
            "Column names: ['Sentence', 'Sentiment']\n",
            "\n",
            "Preprocessing text data...\n",
            "\n",
            "Vectorizing text data...\n",
            "\n",
            "Training the model...\n",
            "\n",
            "Model Performance:\n",
            "Accuracy: 0.6852010265183918\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.35      0.41      0.38       172\n",
            "     neutral       0.77      0.76      0.76       626\n",
            "    positive       0.73      0.69      0.71       371\n",
            "\n",
            "    accuracy                           0.69      1169\n",
            "   macro avg       0.62      0.62      0.62      1169\n",
            "weighted avg       0.70      0.69      0.69      1169\n",
            "\n",
            "\n",
            "Testing with example sentences:\n",
            "Sentence: I love this product, it's amazing!\n",
            "Predicted Sentiment: positive\n",
            "Confidence: 0.99\n",
            "\n",
            "Sentence: The movie was okay, nothing special.\n",
            "Predicted Sentiment: neutral\n",
            "Confidence: 0.91\n",
            "\n",
            "Sentence: The customer service was the worst and I hated it.\n",
            "Predicted Sentiment: neutral\n",
            "Confidence: 0.88\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Simple text preprocessing\n",
        "    \"\"\"\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def create_lexicon():\n",
        "    \"\"\"\n",
        "    Create a simple sentiment lexicon\n",
        "    \"\"\"\n",
        "    positive_words = {'good', 'great', 'awesome', 'excellent', 'happy', 'love', 'wonderful', 'fantastic', 'best', 'amazing'}\n",
        "    negative_words = {'bad', 'terrible', 'awful', 'horrible', 'hate', 'worst', 'poor', 'disappointing', 'disappointed'}\n",
        "    neutral_words = {'okay', 'ok', 'fine', 'average', 'neutral', 'fair', 'moderate'}\n",
        "    return positive_words, negative_words, neutral_words\n",
        "\n",
        "def add_lexicon_features(X, texts):\n",
        "    \"\"\"\n",
        "    Add lexicon-based features to improve accuracy\n",
        "    \"\"\"\n",
        "    positive_words, negative_words, neutral_words = create_lexicon()\n",
        "\n",
        "    # Count sentiment words in each text\n",
        "    positive_counts = []\n",
        "    negative_counts = []\n",
        "    neutral_counts = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = set(text.split())\n",
        "        positive_counts.append(len(words.intersection(positive_words)))\n",
        "        negative_counts.append(len(words.intersection(negative_words)))\n",
        "        neutral_counts.append(len(words.intersection(neutral_words)))\n",
        "\n",
        "    # Add these as new features\n",
        "    X_with_lexicon = np.c_[\n",
        "        X.toarray(),\n",
        "        np.array(positive_counts).reshape(-1, 1),\n",
        "        np.array(negative_counts).reshape(-1, 1),\n",
        "        np.array(neutral_counts).reshape(-1, 1)\n",
        "    ]\n",
        "\n",
        "    return X_with_lexicon\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Print dataset information\n",
        "print(\"Dataset Info:\")\n",
        "print(data.info())\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(data.head())\n",
        "print(\"\\nColumn names:\", data.columns.tolist())\n",
        "\n",
        "\n",
        "# Preprocess the text data\n",
        "print(\"\\nPreprocessing text data...\")\n",
        "X_text = data['Sentence'].apply(preprocess_text)\n",
        "y = data['Sentiment']\n",
        "\n",
        "# Split the data\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create and fit the vectorizer\n",
        "print(\"\\nVectorizing text data...\")\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=3000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "# Transform text data\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "# Add lexicon features\n",
        "X_train_with_lexicon = add_lexicon_features(X_train, X_train_text)\n",
        "X_test_with_lexicon = add_lexicon_features(X_test, X_test_text)\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "model = MultinomialNB(alpha=0.1)  # Slightly reduced smoothing\n",
        "model.fit(X_train_with_lexicon, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_with_lexicon)\n",
        "\n",
        "# Print model performance\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a given text\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Vectorize the text\n",
        "    text_vectorized = vectorizer.transform([processed_text])\n",
        "\n",
        "    # Add lexicon features\n",
        "    text_with_lexicon = add_lexicon_features(text_vectorized, [processed_text])\n",
        "\n",
        "    # Get prediction and probability\n",
        "    prediction = model.predict(text_with_lexicon)[0]\n",
        "    probabilities = model.predict_proba(text_with_lexicon)[0]\n",
        "    confidence = max(probabilities)\n",
        "\n",
        "    return prediction, confidence\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model with some example sentences\n",
        "    test_sentences = [\n",
        "        \"I love this product, it's amazing!\",\n",
        "        \"The movie was okay, nothing special.\",\n",
        "        \"The customer service was the worst and I hated it.\",\n",
        "\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting with example sentences:\")\n",
        "    for sentence in test_sentences:\n",
        "        sentiment, confidence = predict_sentiment(sentence)\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Predicted Sentiment: {sentiment}\")\n",
        "        print(f\"Confidence: {confidence:.2f}\\n\")\n"
      ]
    }
  ]
}